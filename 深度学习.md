# 深度学习

## Transformer

### Transformer简介

是一个有Encoder和Decoder的seq2seq模型。它基于attention机制，使得模型在处理长序列数据时能够更好地捕捉全局依赖关系。最初由Google的团队提出，用于解决机器翻译任务。

### Attention

Attention可以用于处理序列数据的关联性和重要性。在文中使用的是Scaled Dot-Product Attention，是对Query和Key的内积缩放后过一个softmax，再乘以V。
$$
\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

### Self-attention

$\displaystyle\text{Attention}(Q,K,V)=\text{softmax}(\frac{QK^\top}{\sqrt {d_k}})V$，这里Q和K分别是query和key，$\sqrt{d_k}$是缩放因子，让softmax里的内容方差为1。

self attention：计算每个输入的词与其他词的关联性。做法：将input喂入三个fc，生成query, key, value（目的：提升模型的拟合能力，矩阵$W$都是可以训练的，起到一个缓冲的效果）

### Cross attention

cross-attention的输入来自不同的序列，而self-attention的输入则是来自一个单一的嵌入序列。Cross-attention将两个相同维度的嵌入序列不对称地组合在一起，而其中一个序列用作查询Q输入，而另一个序列用作键K和值V输入。

### Multi-head attention

让$V, K, Q$通过不同的$h$组不同的、训练出的线性层，分别通过attention层，再concat在一起。

#### 作用

通过多个注意力头并行计算，模型可以从不同的子空间中学习到更多的特征表示。每个注意力头都可以关注不同的相关性和信息，从而捕捉到输入序列的多个不同方面，增强了模型的表达能力和学习能力。

#### 和CNN相关的地方

和CNN中通过多个卷积核学习多个不同的特征表示有些相似。

### 每一层做什么操作？

首先过一个attention，再通过一个MLP，再做Layer Normalization。

#### 为什么要做层归一化Layer Normalization?

因为输入的seq不一定定长，导致训练的时候每个batch各样本的方差较大，不稳定；测试时使用全局均值和方差，如果遇到特别长的样本则无法处理。每个样本自己做layer normalization，则样本间相对独立。

#### Layer Normalization & Batch Normalization

Batch normalization是在每个特征维度上进行归一化，使得特征的均值接近于0，方差接近于1。训练时使用batch均值和方差，测试时使用全局均值和方差。批归一化适用于卷积层和全连接层等具有多维特征的层。

Layer normalization是对每个样本在特定维度上进行归一化。层归一化适用于RNN等逐步处理序列的层。

归一化好处：减轻输入数据的分布偏移，加速模型收敛速度，提高模型的稳定性和泛化能力，可以缓解梯度问题。

### 如何记录词的位置信息？

将Positional Encoding的信息concat到词的embedding后，一起作为模型的输入信息。

使用sin和cos函数表示，共512维：
$$
PE(pos, 2i)=\sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)\\
PE(pos, 2i+1)=\cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$

### Decoder为什么/怎么做masking?

因为decoder要执行的任务是：given $y_1,\dots,y_i$，预测$y_{i+1}$，为了保证训练任务和测试任务一致，所以要mask掉$y_{i+1},\dots,y_n$。

将这一位加上一个很大的负数，这样softmax以后就会变成0。

### RNN & Transformer?

都用于处理序列信息。

RNN：将上一时刻的输出作为下一时刻的输入

Transformer：通过Attention层获得全局序列信息。

## 什么是Transformer？

是一个有Encoder和Decoder的seq2seq模型，采用了一种基于注意力机制的结构，使得模型在处理长序列数据时能够更好地捕捉全局依赖关系。



![image-20230522010014858](D:\Software\Desktop\2023-NJUAI-open-day-review\fig\image-20230522010014858.png)

[Illustrated Guide to Transformers Neural Network: A step by step explanation - YouTube](https://www.youtube.com/watch?v=4Bdc55j80l8)

- encoder

  - input embedding：为每个词寻找一个向量表征

  - positional encoding：加入位置信息（使用sin和cos函数）

  - encoder layer：输入序列$\to$抽象连续表征

    - multi-head attention + norm：输入：序列，输出：每个单词与其他词的关联度

    - fully connected network + norm：residual connection

- decoder：生成文本序列，autoregressive，masking（只能算和过去词语的attention score）

  - 和encoder相似
  - classifier

Transformer模型的核心思想是通过自注意力机制（Self-Attention）来建立输入序列的表示。自注意力机制能够计算序列中各个位置之间的相关性，并根据相关性分配不同的权重，从而更好地捕捉上下文信息。通过多层堆叠的自注意力层和前馈神经网络层，Transformer能够对输入序列进行逐层的特征提取和组合，从而获得更丰富的表示。

Transformer模型的另一个重要组成部分是位置编码（Positional Encoding），用于将序列中每个位置的信息嵌入到表示中，以考虑序列的顺序信息。位置编码可以告知模型每个输入在序列中的位置，从而在没有显式顺序信息的情况下仍能正确建模序列。

由于其出色的性能和可扩展性，Transformer模型不仅在机器翻译任务中表现出色，还被广泛应用于其他NLP任务，如文本生成、命名实体识别、情感分析等。此外，Transformer的思想也被拓展到其他领域，如计算机视觉（如图像生成和图像分类）和推荐系统等，取得了很好的效果。



## 什么是LSTM？

[人人都能看懂的LSTM - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/32085405)

LSTM就是long short-term memory，是一种循环神经网络的变体，通过引入“记忆单元”来解决长序列训练过程中的梯度消失和梯度爆炸问题。

- 相比RNN只有一个传递状态$h^t$，LSTM有两个传输状态，一个$c^t$（cell state），和一个$h^t$（hidden state）。其中$c^t$通常是$c^{t-1}$加上一些值，改变得很慢；$h^t$在不同结点下往往有很大区别。
- 加上了门控状态：
  - 忘记：$z^f$ (f for forget)，来控制上一个状态的$c^{t-1}$哪些留哪些忘
  - 选择记忆：$z^i$ (i for information)，对$x^t$进行选择性的记忆。$z$作为输入
  - 输出：决定哪些将会被当成状态的输出，用$z^o$来进行控制，并且还对上一阶段得到的$c^0$进行了放缩。

![img](D:\Software\Desktop\review\fig\v2-e4f9851cad426dfe4ab1c76209546827_1440w.webp)

![img](https://pic1.zhimg.com/80/v2-d044fd0087e1df5d2a1089b441db9970_1440w.webp)

![img](https://pic1.zhimg.com/80/v2-d044fd0087e1df5d2a1089b441db9970_1440w.webp)

![img](https://pic2.zhimg.com/v2-556c74f0e025a47fea05dc0f76ea775d_r.jpg)

## 什么是CNN？

CNN / convolutional neural networks / 卷积神经网络：用kernel对输入进行局部的特征提取

与传统的神经网络相比，CNN更加适合处理图像和语音等高维数据。

CNN是一种前向反馈神经网络，主要由卷积层、池化层和全连接层等组成。在卷积层中，CNN通过卷积核对输入数据进行卷积运算，从而提取特征。卷积核可以在不同的位置进行卷积运算，这样可以检测出输入数据中的不同特征。在池化层中，CNN通过对卷积层输出进行下采样，从而降低数据维度，同时也可以减小数据量，提高模型的训练速度。在全连接层中，CNN将池化层输出的特征映射到输出层中，完成分类或回归任务。

## 什么是MLP？

MLP / multi-layer perception / 多层感知器：除了输入输出层还有隐层

## 什么是GLU？

GLU / Gated Linear Unit / 门控线性单元：$GLU(x,W,V,b,c)=\sigma(xW+b)\otimes (xV+c)$，其中$\sigma$是Sigmoid函数，$\otimes$是矩阵逐点相乘，总的来说效果比ReLU好。