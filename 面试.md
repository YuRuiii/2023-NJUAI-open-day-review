### 机器学习

- 机器学习：基于统计、数据驱动，不是基于演绎而是基于归纳的。可以解决规则难以解决的问题；深度学习：使用更深的网络结果增强表达能力，将特征提取和模式学习相结合
- 梯度下降法：一种常用的优化算法，用于最小化目标函数。计算过程是迭代地向梯度下降最快的方向更新参数，新参数=旧参数-学习率$\times$梯度。
  - 条件：目标函数可导，凸；优点：应用范围广，简单；缺点：可能陷入局部最小值，需要人为确定学习率
- 牛顿迭代法：求$f(x)=0$的解的方法。每次用切线与x轴的交点的x值更新参数，$\displaystyle x_{n+1}=x_n-\frac{f(x_n)}{f'(x_n)}$
  - 条件：目标函数二价可微，目标函数的海森矩阵必须正定或负定
  - 优点：更快地收敛到最优解，不用调参；缺点：计算成本高
- 决策树：一种基于树结构进行决策的算法，其中每个内部节点表示一个属性或特征，每个叶子节点代表决策结果。该算法可以用于分类和回归问题。
  - 计算过程：基于分治思想，每次选择最优划分属性，将样本集合分成更小的子集。不断重复，直到所有的子集都属于同一个类别，或者已经达到了预先定义的最大深度。
- SVM (Support Vector Machine): 支持向量机，一种可以用于解决分类和回归问题的机器学习算法，目标是在高维空间中找到最佳的超平面，使得间隔最大化。
  - 支持向量：离超平面最近的训练样本点。
  - SVM为什么要化对偶形？为了更高效地求解。当数据集的规模很大时，使用原始问题求解SVM会变得很慢，甚至变得不可行。化成对偶问题可以使用SMO这种近似算法更高效的求解。
  - 百万样本量可以用SVM吗？求解SVM的原问题计算复杂度过大，有些困难。
- 朴素贝叶斯：一种采用了属性条件独立性假设的分类器，基于贝叶斯定理，选择概率最大的类作为结果。
- 贝叶斯最优分类器：为最小化总体风险，在每个样本上选择那个能使条件风险最小的类别标记，此时判定准则称为贝叶斯最优分类器
- EM算法：一种迭代地估计参数隐变量的方法。其中E步 (Expectation) 基于参数推断隐变量的分布；M步 (Maximization) 基于已观测变量X和Z对参数做极大似然估计。
- PCA (Principal Component Analysis)：无监督降维方法。
  - 适用于数据之间存在线性相关性的情况，对于非线性关系可以考虑使用Kernel PCA。
  - 计算过程：先将数据标准化，使得每个特征的均值为0；计算标准化后数据的协方差矩阵，对协方差矩阵做特征值分解，选择特征值最大的k个特征向量组成投影矩阵，最后将标准化后的数据乘以投影矩阵，将数据投影到新的特征空间上。
  - 计算后的主成分正交，即没有线性相关性。（正交$\Rightarrow$线性无关）
- 集成学习 (ensemble learning，读“昂”)：一种机器学习方法，核心思想是通过组合多个弱学习器，从而提高模型的预测能力和泛化性能。
  - Bagging (bootstrap aggregating)：使用自助采样法得到不同的训练集，训练一系列基学习器，通过投票或者平均的方法得到预测结果。
  - Boosting：基本思想是通过迭代训练一系列弱分类器，每个弱分类器都针对前一轮训练中被错误分类的样本进行加权，以提高其分类准确性。
  - Stacking：一种叠加泛化的集成方法，通过训练一个元模型来结合多个基础模型的预测结果。元模型就是以不同基学习器的预测结果作为输入，结果作为输出。
  - “好而不同”：即基学习器的准确率高，多样性高。直观讲准确率越高越好是显然的，多样性高意味着不同的学习器可以互补，从而提高整体的学习性能。理论上有误差歧义分解证明了泛化误差=基学习器泛化误差的却权均值-基学习器的加权分歧值，可以说明基学习器的准确率高，多样性高，分类效果越好。
  - Adaboost计算过程：（二分类任务）在训练的每一轮中，使用当前的样本权重训练基学习器，计算分类误差率，如果误差大于0.5则break；根据错误率计算这个基学习器的权重；基于对分类正误情况更新数据分布，增加分错样本的影响，降低分对样本的影响；最后基于各个基学习器的加权，得到最终的分类器
- K-means：一种原型聚类方法，目标是将数据集分成k个不重叠的簇。通过最小化数据点与所属簇的质心之间的距离来进行聚类。
  - 计算过程：用一种迭代近似的方法求解。初始化k个簇的质心，重复如下步骤：将每个数据点分到与其最近的质心所属的簇；对于每个簇，计算所有数据点的均值，得到新的质心。如此重复直到质心的位置不再发生显著变化或达到预定的迭代次数。
- KNN：一种lazy learning的监督学习算法，基于与测试样本最近邻的k个邻居的信息预测结果。
  - lazy learning：训练阶段仅仅是把样本保存起来，训练时间开销为零，待 收到测试样本后再进行处理
- PAC (Probably Approximately Correct)：一种计算学习理论，核心思想是如果一个算法能够在有限的样本上以很高的概率产生一个"近似正确"的模型，那么该算法可以在未见过的数据上表现良好。
  - PAC可辨识：算法是否能以高概率产生一个近似正确的模型（泛化误差小于epsilon的概率大于等于1-delta，则称学习算法能从假设空间中PAC辨识概念类C）
  - PAC可学习：在有限样本下算法是否能够以高概率产生一个近似正确的模型
  - PAC学习算法：在有限的样本和时间限制下，学习算法是否能够以高概率产生一个近似正确的模型

### 线代

- 秩：矩阵中线性无关的行或列的最大数量。矩阵的秩可以看作是矩阵所代表的线性变换值域的向量空间的维度，比如说PCA中的协方差矩阵如果秩是k，那么数据可以被无损压缩到k维。
- 线性相关：存在一组不全为零的标量（系数），使得向量之间的线性组合等于零向量。
- 特征值，特征向量：对于n阶矩阵$A$，常数$\lambda$，和非零的n​维列向量$\alpha$，有$A \alpha=\lambda \alpha$成立，则称$\lambda$是矩阵A的一个特征值，$\alpha$为矩阵$A$属于特征值$\lambda$的特征向量。
- 特征值分解：将一个有$n$个线性无关的特征向量的$n$阶方阵$A$分解为$A=Q \Lambda Q^{-1}$的过程。其中$Q$中的每一列都是特征向量，Lambda为对角矩阵，对角线上的元素是对应的特征值。
  - 首先求解特征方程$|\lambda E-A|=0$，得到矩阵$A$特征值$\lambda _i$（共$n$个）
  - 再由$(\lambda_i E-A)x=0$求基础解系，即矩阵$A$属于特征值$\lambda_i$的线性无关的特征向量。
  - 用求得的特征值和特征向量构造Q，Lambda​
- 奇异值分解：将矩阵A分解为$U \Sigma V^{-1}$，其中U V均为正交矩阵，Sigma为对角矩阵，对角线上的值按从大到小排列。
  - 计算：对$AA^\top$、$A^\top A$分别进行特征值分解，得到U和V。两次计算中的非零特征值从大到小排序，放在对角线，其他位置为零。
- 矩阵分解的意义：将一个矩阵表示为多个矩阵的乘积的过程，有比如说奇异值分解，特征值分解。可以将复杂的矩阵问题简化为更易处理的子问题，提高计算效率。可以提取矩阵中的隐藏模式和结构，用于数据的降维、特征提取和模式识别等任务。
- 正定：实对称矩阵$A$，对任何非零向量$x$，都满足$x^\top  A x>0$

  - 充要条件：实对称矩阵 $A$ 正定 $\Leftrightarrow A$ 与单位矩阵 $E$ 合同；存在可逆矩阵 $C$，使 $A=C^\top  C$；$A$的特征值均为正；正惯性指数$p=n$；$A$ 的各阶顺序主子式都大于零
  - 性质：$A^{-1}, kA,A^*,A^m, (A+B)$也是正定矩阵
  - 必要条件：实对称矩阵 $A$ 正定 $\Rightarrow a_{ii}>0$；$\det A=|A|>0$
- 半正定：实对称矩阵$A$，对任何非零向量$x$，都满足$x^\top  A x\geq0$

  - 等价命题：$f(x_1,x_2,\dots,x_n)$半正定
  - $A$半正定
  - $r(f)=r(A)=p$（正惯性指数）
  - $A$合同于非负对角阵，即存在可逆阵$C$，使得$C^\top AC=\text{diag} (d_1,\dots,d_n),d_i\geq 0$
  - 存在方阵$G\in R^{n\times n}$，使$A=G^\top G$
  - 所有主子式$\geq0$
- 梯度消失和梯度爆炸是什么意思？有什么样的方法进行改善？梯度消失：残差连接（输出为输入和输出的求和），ReLU；梯度爆炸：归一化


### 概率论

- 大数定律：随机变量的均值依概率收敛于期望的均值。
- 中心极限定理：在适当的条件下，大量相互独立随机变量的均值依分布收敛于正态分布。
- 局限：样本容量要求，可以用集中不等式解决。
- 贝叶斯公式：用于计算在某个先验条件下事件的后验概率的公式。
- 全概率公式：一个事件发生的概率等于它在与样本空间的一个划分中每个事件同时发生的概率之和。
- 最大似然估计：是一种用于估计模型参数的统计方法。极大似然估计的基本思想是找到使观测数据的似然函数最大化的参数值。

  - 需要满足：样本独立同分布，参数化假设（参数可数且取值有限），模型合适
- 独立同分布：independent and identically distributed，即独立同分布假设，假设数据集中的每个样本都是从同一个概率分布中独立地随机取出的。目的是简化建模。
- 高维高斯分布：$\displaystyle p(x)=(2\pi)^{- D/2}|\Sigma|^{-1/2}\exp\{-\frac12(x-\mu)^{\top}\Sigma ^{-1}(x-\mu)\}$

### 数据结构与算法

- P问题：可以在多项式时间内求解的决策问题。
- NP问题：可以在多项式时间内验证答案是否正确的决策问题。
- NP-complete ：NP集合中最难的问题。是NP，且所有NP问题都可以在多项式时间内规约到它的问题。
- NP-hard ：所有NP问题都可以在多项式时间内规约到它的问题。
- 排序稳定性：对于值相同的两个元素，排序后它们的先后顺序与原数组相同。比如在有些使用场景下需要保留原数据中元素的相对位置。
- 快排：是一个基于分治的思想的排序算法，每次选择一个元素作为pivot，然后将给定的数组围绕这个基准进行划分，将小于基准的元素放到基准的左边，将大于基准的元素放到基准的右边，最终将基准放置在排序后的正确位置上。如此递归。优点：空间复杂度小，缺点：最坏复杂度无保证。选快排的原因：快排不需要进行分配和取消分配辅助数组等操作，其所需的常量时间小；空间复杂度低。
- 归并排序：基于分治的思想的排序算法，每次将数组分为两个子数组，分别对子数组排序，再将两个子数组归并，形成最终的排序好的数组。优点：可并行；缺点：空间复杂度高。
- topK：维护一个大小为k的小顶堆，时间复杂度$O(n\log k)$；类似快排，比如说找最大的数，pivot划分后左边80个，右边20个，只要对右边排序即可。最优$O(n)$，平均$O(n\log n)$
- 判断图中有环：
  - 拓扑排序，最后如果还存在未被删除的顶点，则表示有环；否则没有环
  - DFS，如果在遍历的过程中，发现某个节点有一条边指向已经访问过的节点，并且这个已访问过的节点不是当前节点的父节点，则表示存在环（用黑白灰表示）
  - 对于有向图，若存在强连通分量（即存在$v_i \to v_j$的路径也存在$v_j\to v_i$的路径），图中有环。
- 欧拉图：具有欧拉回路的图，欧拉回路是通过图中每条边恰好一次的回路
  - 有向图：非零度顶点是强连通的；每个顶点的入度和出度相等
  - 无向图是欧拉图当且仅当：非零度顶点是连通的；顶点的度数都是偶数
- 霍夫曼编码：一种变长的数据压缩编码，通过让出现概率更小的字符长度更长，出现概率更大的字符长度更短进行数据压缩，是最优编码。基于霍夫曼树构造的。
  - 每次取出出现概率最小的两个结点，合并后结点权重为两个结点之和。直到所有结点构成一个二叉树。为二叉树的每个结点下的两个分支赋值为0、1，编码即从根节点到字符的叶节点的路径上01串。
  - 唯一可译码：因为霍夫曼编码是前缀编码，即没有一个编码是另一个编码的前缀。
  - 优点：最优编码。缺点：与计算机的数据结构不匹配；需要多次排序，耗费时间。
- 香农编码：一种变长的数据压缩编码，效率略低于霍夫曼。
  - 编码方法：将符号按照其出现概率从大到小排序；把概率集合分成两个尽可能概率相等的子集，给前面一个子集合赋值为0，后面一个子集合赋值为1；如此重复直到各个子集合中只有一个元素为止；将每个元素所属的子集合的值依次串起来
- 散列表的优势：查询高效，数据的存储和删除也高效。
  - 查询复杂度：链表型：每个哈希桶维护一个链表来存储冲突的键值对，最坏是O(n)​，平均接近O(1)​
  - 普通型：当发生哈希冲突时，开放寻址再找到一个空的位置来存储冲突的值。最坏情况是O(n)（哈希表被填满），平均接近O(1)
- 斐波那契：$O(2^n)$，存储结果$O(n)$；动态规划$O(n)$

### 其他

- 操作系统线程跟进程的区别：进程是一个独立的程序运行的实例，线程是进程中的一个子任务。进程可以包含多个线程，进程之间是独立的，
- 傅里叶变换：将信号从时域转换到频域的变换过程。
- 拉普拉斯变换：傅里叶变换的一种扩展，可以将信号从时域转换到复频域。
  - 用处：在控制系统中，分析设计反馈控制系统；在信号处理中，用于滤波、信号重构、频谱估计和图像处理等
  - 两者关系：拉普拉斯变换是傅里叶变换的扩展形式；在频域分析中，拉普拉斯变换通常用于处理连续系统的输入-输出关系和系统响应，而傅里叶变换适用于连续信号和离散信号。
- 凸优化带约束的一阶条件：定义域是凸集，$f(y)> f(x)+\nabla f(x)^\top (y-x)$
- 二阶条件：对于开集内的任意一点，它的Hessian矩阵半正定。$\nabla^2 f(x)\geq 0$
- KKT条件是非线性规划中最优解的一阶必要条件：拉格朗日函数的导数为0，原始可行性条件，对偶可行性条件，互补松弛条件。
  - 原始可行性条件：优化变量满足原问题的所有的约束条件，包括对于每个不等式约束和等式约束
  - 对偶可行性条件：对于每个不等式约束，对应的拉格朗日乘子大于等于0。
  - 互补松弛条件：约束的松弛变量与拉格朗日乘子的乘积为零
- 凸优化中梯度下降的学习率：通过线搜索确定。线搜索是一种通过搜索确定每次迭代的合适学习率的方法。在每次迭代中，沿着梯度方向尝试不同的学习率值，通过一些准则来选择合适的学习率，使得目标函数在新的参数值上有足够的下降。精确，计算成本高。
- 如果函数的定义域是凸集，且对于任意定义域上的$x,y$和任意$0\leq \theta\leq 1$，有$f(\theta x+(1-\theta)y)\leq \theta f(x)+(1-\theta)f(y)$，则函数是凸的。

- 卷积和：将其中一个信号翻转再平移，再和另一个向量相乘，对乘积后的信号求和。
- A*：树admissible则optimal，图consistent则optimal。admissible即到目标的估计成本<到目标的真实成本，consistent即结点与其后继的估计成本之差<这一步的真实成本

- 原问题为什么要化成对偶问题：原问题线性可分就即凸，线性不可分则不凸

- 对偶问题的求解可以用SMO等近似方法求解。可以引入核函数。（通过迭代地优化一对变量来求解SVM的优化问题。其主要思想是选择两个变量进行优化，将其他变量固定，并通过解析方法直接计算这两个变量的最优值。通过迭代地更新这对变量，SMO算法逐渐逼近全局最优解。）

- 斐波那契数列求和：公式，DP，递归（拿字典存）

- 双蛋问题：可以用动态规划求解，

  $W(n,h)$表示有n个鸡蛋h层楼所需的最少实验次数
  $$
  W(n,h)=1+\min(\max(W(n-1, i-1), W(n, h-i)))
  $$
  $n=2$：$W(h)=1+\min(\max(i-1, W(h-i)))$

- 激活函数：引入非线性特性，Sigmoid，Tanh，ReLU，LeakyReLU。Sigmoid映射到(0,1)，Tanh映射到(-1,1)，这两个激活函数都存在梯度消失。ReLU可以一定缓解梯度消失问题，但是在负输入时不可导，可能存在神经元死亡的情况。LeakyReLU对ReLU改进，在负输入时保留一个小的梯度，有助于解决ReLU函数中的死亡神经元问题。

- 特征值特征向量意义：特征值告诉我们特征向量在线性变换中的伸缩程度，PCA中特征值越大说明对应的特征向量相关性更大，取最大的k维。特征值可以用来判断矩阵的正定性等（特征值均>0）。
- 数据规模大：对偶问题太复杂。
